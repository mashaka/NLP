{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Deep learning for Natural Language Processing\n",
    "\n",
    "\n",
    " * Simple text representations, bag of words\n",
    " * Word embedding and... not just another word2vec this time\n",
    " * rnn for text\n",
    " * Aggregating several data sources \"the hard way\"\n",
    " * Solving ~somewhat~ real ML problem with ~almost~ end-to-end deep learning\n",
    " \n",
    "\n",
    "Special thanks to Irina Golzmann for help with technical part, task prepared by Александр Панин, jheuristic@yandex-team.ru"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# NLTK\n",
    "\n",
    "You will require nltk v3.2 to solve this assignment\n",
    "\n",
    "__It is really important that the version is 3.2, otherwize russian tokenizer might not work__\n",
    "\n",
    "Install/update\n",
    "* `sudo pip install --upgrade nltk==3.2`\n",
    "* If you don't remember when was the last pip upgrade, `sudo pip install --upgrade pip`\n",
    "\n",
    "If for some reason you can't or won't switch to nltk v3.2, just make sure that russian words are tokenized properly with RegeExpTokenizer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# For students with low-RAM machines\n",
    " * This assignment can be accomplished with even the low-tier hardware (<= 4Gb RAM) \n",
    " * If that is the case, turn flag \"low_RAM_mode\" below to True\n",
    " * If you have around 8GB memory, it is unlikely that you will feel constrained by memory.\n",
    " * In case you are using a PC from last millenia, consider setting very_low_RAM=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "low_RAM_mode = False\n",
    "very_low_RAM = False  #If you have <3GB RAM, set BOTH to true"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Dataset\n",
    "\n",
    "Ex-kaggle-competition on prohibited content detection\n",
    "\n",
    "There goes the description - https://www.kaggle.com/c/avito-prohibited-content\n",
    "\n",
    "\n",
    "### Download\n",
    "High-RAM mode,\n",
    " * Download avito_train.tsv from competition data files\n",
    "Low-RAM-mode,\n",
    " * Download downsampled dataset from here\n",
    "     * archive https://yadi.sk/d/l0p4lameqw3W8\n",
    "     * raw https://yadi.sk/d/I1v7mZ6Sqw2WK (in case you feel masochistic)\n",
    " \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# What's inside\n",
    "Different kinds of features:\n",
    "* 2 text fields - title and description\n",
    "* Special features - price, number of e-mails, phones, etc\n",
    "* Category and subcategory - unsurprisingly, categorical features\n",
    "* Attributes - more factors\n",
    "\n",
    "Only 1 binary target whether or not such advertisement contains prohibited materials\n",
    "* criminal, misleading, human reproduction-related, etc\n",
    "* diving into the data may result in prolonged sleep disorders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "if not low_RAM_mode:\n",
    "    # a lot of ram\n",
    "    df = pd.read_csv(r\"avito_train.tsv\",sep='\\t')\n",
    "else:\n",
    "    #aroung 4GB ram\n",
    "    df = pd.read_csv(r\"avito_train_1kk.tsv\",sep='\\t')\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3995803, 13) 0.0688212106553\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>itemid</th>\n",
       "      <th>category</th>\n",
       "      <th>subcategory</th>\n",
       "      <th>title</th>\n",
       "      <th>description</th>\n",
       "      <th>attrs</th>\n",
       "      <th>price</th>\n",
       "      <th>is_proved</th>\n",
       "      <th>is_blocked</th>\n",
       "      <th>phones_cnt</th>\n",
       "      <th>emails_cnt</th>\n",
       "      <th>urls_cnt</th>\n",
       "      <th>close_hours</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10000010</td>\n",
       "      <td>Транспорт</td>\n",
       "      <td>Автомобили с пробегом</td>\n",
       "      <td>Toyota Sera, 1991</td>\n",
       "      <td>Новая оригинальная линзованая оптика на ксенон...</td>\n",
       "      <td>{\"Год выпуска\":\"1991\", \"Тип кузова\":\"Купе\", \"П...</td>\n",
       "      <td>150000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10000025</td>\n",
       "      <td>Услуги</td>\n",
       "      <td>Предложения услуг</td>\n",
       "      <td>Монтаж кровли</td>\n",
       "      <td>Выполняем  монтаж кровли фальцевой ^p Тел:8@@P...</td>\n",
       "      <td>{\"Вид услуги\":\"Ремонт, строительство\"}</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>22.38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10000094</td>\n",
       "      <td>Личные вещи</td>\n",
       "      <td>Одежда, обувь, аксессуары</td>\n",
       "      <td>Костюм Steilmann</td>\n",
       "      <td>Юбка и топ из панбархата. Под топ  трикотажная...</td>\n",
       "      <td>{\"Вид одежды\":\"Женская одежда\", \"Предмет одежд...</td>\n",
       "      <td>1500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.41</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10000101</td>\n",
       "      <td>Транспорт</td>\n",
       "      <td>Автомобили с пробегом</td>\n",
       "      <td>Ford Focus, 2011</td>\n",
       "      <td>Автомобиль в отличном техническом состоянии, в...</td>\n",
       "      <td>{\"Марка\":\"Ford\", \"Модель\":\"Focus\", \"Год выпуск...</td>\n",
       "      <td>365000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>8.87</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>10000132</td>\n",
       "      <td>Транспорт</td>\n",
       "      <td>Запчасти и аксессуары</td>\n",
       "      <td>Турбина 3.0 Bar</td>\n",
       "      <td>Продам турбину на двигатель V-6 . V-8 и мощнее...</td>\n",
       "      <td>{\"Вид товара\":\"Запчасти\", \"Тип товара\":\"Для ав...</td>\n",
       "      <td>5000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>11.82</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     itemid     category                subcategory              title  \\\n",
       "0  10000010    Транспорт      Автомобили с пробегом  Toyota Sera, 1991   \n",
       "1  10000025       Услуги          Предложения услуг      Монтаж кровли   \n",
       "2  10000094  Личные вещи  Одежда, обувь, аксессуары   Костюм Steilmann   \n",
       "3  10000101    Транспорт      Автомобили с пробегом   Ford Focus, 2011   \n",
       "4  10000132    Транспорт      Запчасти и аксессуары    Турбина 3.0 Bar   \n",
       "\n",
       "                                         description  \\\n",
       "0  Новая оригинальная линзованая оптика на ксенон...   \n",
       "1  Выполняем  монтаж кровли фальцевой ^p Тел:8@@P...   \n",
       "2  Юбка и топ из панбархата. Под топ  трикотажная...   \n",
       "3  Автомобиль в отличном техническом состоянии, в...   \n",
       "4  Продам турбину на двигатель V-6 . V-8 и мощнее...   \n",
       "\n",
       "                                               attrs   price  is_proved  \\\n",
       "0  {\"Год выпуска\":\"1991\", \"Тип кузова\":\"Купе\", \"П...  150000        NaN   \n",
       "1             {\"Вид услуги\":\"Ремонт, строительство\"}       0        NaN   \n",
       "2  {\"Вид одежды\":\"Женская одежда\", \"Предмет одежд...    1500        NaN   \n",
       "3  {\"Марка\":\"Ford\", \"Модель\":\"Focus\", \"Год выпуск...  365000        NaN   \n",
       "4  {\"Вид товара\":\"Запчасти\", \"Тип товара\":\"Для ав...    5000        NaN   \n",
       "\n",
       "   is_blocked  phones_cnt  emails_cnt  urls_cnt  close_hours  \n",
       "0           0           0           0         0         0.03  \n",
       "1           0           1           0         0        22.38  \n",
       "2           0           0           0         0         0.41  \n",
       "3           0           0           0         0         8.87  \n",
       "4           0           0           0         0        11.82  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(df.shape, df.is_blocked.mean())\n",
    "df[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "![caption](https://kaggle2.blob.core.windows.net/competitions/kaggle/3929/media/Ad.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Blocked ratio 0.0688212106553\n",
      "Blocked Count: 3720807\n",
      "Count: 3995803\n"
     ]
    }
   ],
   "source": [
    "print(\"Blocked ratio\", df.is_blocked.mean())\n",
    "print(\"Blocked Count:\", len(df[df.is_blocked == 0]))\n",
    "print(\"Count:\", len(df))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Balance-out the classes\n",
    "* Vast majority of data samples are non-prohibited\n",
    " * 250k banned out of 4kk\n",
    " * Let's just downsample random 250k legal samples to make further steps less computationally demanding\n",
    " * If you aim for high Kaggle score, consider a smarter approach to that.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Blocked ratio: 0.50852861116\n",
      "Count: 540768\n"
     ]
    }
   ],
   "source": [
    "# downsample data so that both classes have approximately equal ratios>\n",
    "\n",
    "n = int(round((1 - df.is_blocked.mean()) / df.is_blocked.mean()))\n",
    "df = pd.concat([df[df.is_blocked == 1], df[df.is_blocked == 0].iloc[::n, :]])\n",
    "\n",
    "print(\"Blocked ratio:\", df.is_blocked.mean())\n",
    "print(\"Count:\", len(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All tests passed\n"
     ]
    }
   ],
   "source": [
    "assert(df.is_blocked.mean() < 0.51)\n",
    "assert(df.is_blocked.mean() > 0.49)\n",
    "assert(len(df) <= 560000)\n",
    "\n",
    "print(\"All tests passed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Tokenizing\n",
    "\n",
    "First, we create a dictionary of all existing words.\n",
    "Assign each word a number - it's Id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from nltk.tokenize import RegexpTokenizer\n",
    "from collections import Counter, defaultdict\n",
    "tokenizer = RegexpTokenizer(r\"\\w+\")\n",
    "\n",
    "#Dictionary of tokens\n",
    "token_counts = Counter()\n",
    "\n",
    "#All texts\n",
    "all_texts = np.hstack([df.description.values, df.title.values])\n",
    "\n",
    "\n",
    "#Compute token frequencies\n",
    "for s in all_texts:\n",
    "    if type(s) is not str:\n",
    "        continue\n",
    "    s = s.lower()\n",
    "    tokens = tokenizer.tokenize(s)\n",
    "    for token in tokens:\n",
    "        token_counts[token] +=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Remove rare tokens\n",
    "\n",
    "We are unlikely to make use of words that are only seen a few times throughout the corpora.\n",
    "\n",
    "Again, if you want to beat Kaggle competition metrics, consider doing something better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAY0AAAD8CAYAAACLrvgBAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nQ0OS1O3/AJe/pwsPLu1uAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f88f8d77ef0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Word frequency distribution, just for kicks\n",
    "_ = plt.hist(list(token_counts.values()), range=[0,50], bins=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#Select only the tokens that had at least 10 occurences in the corpora.\n",
    "#Use token_counts.\n",
    "\n",
    "min_count = 10\n",
    "tokens = [k for k, v in token_counts.items() if v >= min_count]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "token_to_id = {t: i+1 for i, t in enumerate(tokens)}\n",
    "null_token = \"NULL\"\n",
    "token_to_id[null_token] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Tokens before: 516253\n",
      "# Tokens after filtering: 87072\n"
     ]
    }
   ],
   "source": [
    "print(\"# Tokens before:\", len(token_counts))\n",
    "print(\"# Tokens after filtering:\", len(token_to_id))\n",
    "if len(token_to_id) < 30000:\n",
    "    print(\"Alarm! It seems like there are too few tokens. Make sure you updated NLTK and applied correct thresholds -- unless you now what you're doing, ofc\")\n",
    "if len(token_to_id) > 1000000:\n",
    "    print(\"Alarm! Too many tokens. You might have messed up when pruning rare ones -- unless you know what you're doing, ofc\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Replace words with IDs\n",
    "Set a maximum length for titles and descriptions.\n",
    " * If string is longer that that limit - crop it, if less - pad with zeros.\n",
    " * Thus we obtain a matrix of size [n_samples] x [max_length]\n",
    " * Element at i, j - is an identifier of word j within sample i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def vectorize(strings, token_to_id, max_len=150):\n",
    "    token_matrix = []\n",
    "    for s in strings:\n",
    "        if type(s) is not str:\n",
    "            token_matrix.append([0] * max_len)\n",
    "            continue\n",
    "        s = s.lower()\n",
    "        tokens = tokenizer.tokenize(s)\n",
    "        token_ids = list(map(lambda token: token_to_id.get(token, 0), tokens))[:max_len]\n",
    "        token_ids += [0] * (max_len - len(token_ids))\n",
    "        token_matrix.append(token_ids)\n",
    "\n",
    "    return np.array(token_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "desc_tokens = vectorize(df.description.values, token_to_id, max_len = 150)\n",
    "title_tokens = vectorize(df.title.values, token_to_id, max_len = 15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Data format examples\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Размер матрицы: (540768, 15)\n",
      "Поездки на таможню, печать в паспорте -> [82315 37963 17853 74611 86214  2079     0     0     0     0] ...\n",
      "Рефлекторно-урогинекологический массаж -> [ 5907     0 19997     0     0     0     0     0     0     0] ...\n",
      "Возьму суду под200 т. р -> [ 6350 30033     0 37412 58617     0     0     0     0     0] ...\n"
     ]
    }
   ],
   "source": [
    "print(\"Размер матрицы:\", title_tokens.shape)\n",
    "for title, tokens in zip(df.title.values[:3], title_tokens[:3]):\n",
    "    print(title, '->', tokens[:10], '...')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "__ As you can see, our preprocessing is somewhat crude. Let us see if that is enough for our network __"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Non-sequences\n",
    "\n",
    "\n",
    "Some data features are not text samples. E.g. price, # urls, category, etc\n",
    "\n",
    "They require a separate preprocessing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# All numeric features\n",
    "df_numerical_features = df[[\"phones_cnt\",\"emails_cnt\",\"urls_cnt\",\"price\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# One-hot-encoded category and subcategory\n",
    "\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "\n",
    "data_cat_subcat = df[[\"category\",\"subcategory\"]].values\n",
    "\n",
    "# A list of dictionaries {\"category\":category_name, \"subcategory\":subcategory_name} for each data sample\n",
    "categories = [{\"category\": v[0], \"subcategory\": v[1]} for v in data_cat_subcat]\n",
    "\n",
    "vectorizer = DictVectorizer(sparse=False)\n",
    "cat_one_hot = vectorizer.fit_transform(categories)\n",
    "cat_one_hot = pd.DataFrame(cat_one_hot, columns=vectorizer.feature_names_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "df_non_text = pd.merge(\n",
    "    df_numerical_features, cat_one_hot, on = np.arange(len(cat_one_hot))\n",
    ")\n",
    "del df_non_text[\"key_0\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Split data into training and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#Target variable - whether or not sample contains prohibited material\n",
    "target = df.is_blocked.values.astype('int32')\n",
    "#Preprocessed titles\n",
    "title_tokens = title_tokens.astype('int32')\n",
    "#Preprocessed tokens\n",
    "desc_tokens = desc_tokens.astype('int32')\n",
    "\n",
    "#Non-sequences\n",
    "df_non_text = df_non_text.astype('float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#Split into training and test set.\n",
    "\n",
    "#Difficulty selector:\n",
    "#Easy: split randomly\n",
    "#Medium: select test set items that have item_ids strictly above that of training set\n",
    "#Hard: do whatever you want, but score yourself using kaggle private leaderboard\n",
    "\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "title_tr,title_ts,desc_tr,desc_ts,nontext_tr,nontext_ts,target_tr,target_ts = train_test_split(\n",
    "    title_tokens, desc_tokens, df_non_text, target, test_size=0.2, random_state=17)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Train the monster\n",
    "\n",
    "Since we have several data sources, our neural network may differ from what you used to work with.\n",
    "\n",
    "* Separate input for titles: RNN\n",
    "* Separate input for description: RNN\n",
    "* Separate input for categorical features: обычные полносвязные слои или какие-нибудь трюки\n",
    " \n",
    "These three inputs must be blended somehow - concatenated or added.\n",
    "\n",
    "* Output: a simple binary classification\n",
    " * 1 sigmoidal with binary_crossentropy\n",
    " * 2 softmax with categorical_crossentropy - essentially the same as previous one\n",
    " * 1 neuron without nonlinearity (lambda x: x) +  hinge loss\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using gpu device 0: Tesla K80 (CNMeM is disabled, cuDNN 5103)\n",
      "/usr/local/lib/python3.4/site-packages/Theano-0.8.2-py3.4.egg/theano/sandbox/cuda/__init__.py:600: UserWarning: Your cuDNN version is more recent than the one Theano officially supports. If you see any problems, try updating Theano or downgrading cuDNN to version 5.\n",
      "  warnings.warn(warn)\n"
     ]
    }
   ],
   "source": [
    "#libraries\n",
    "import lasagne\n",
    "from theano import tensor as T\n",
    "import theano"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#3 inputs and a refere output\n",
    "title_token_ids = T.matrix(\"title_token_ids\", dtype='int32')\n",
    "desc_token_ids = T.matrix(\"desc_token_ids\", dtype='int32')\n",
    "categories = T.matrix(\"categories\", dtype='float32')\n",
    "target_y = T.ivector(\"is_blocked\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# NN architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "title_inp = lasagne.layers.InputLayer((None,title_tr.shape[1]),input_var=title_token_ids)\n",
    "descr_inp = lasagne.layers.InputLayer((None,desc_tr.shape[1]),input_var=desc_token_ids)\n",
    "cat_inp = lasagne.layers.InputLayer((None,nontext_tr.shape[1]), input_var=categories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Descriptions\n",
    "\n",
    "HIDDEN_UNITS = 64\n",
    "GRAD_CLIP = 100\n",
    "\n",
    "#word-wise embedding. We recommend to start from some 64 and improving after you are certain it works.\n",
    "descr_emb = lasagne.layers.EmbeddingLayer(descr_inp, input_size=len(token_to_id)+1, output_size=128)\n",
    "descr_lstm = lasagne.layers.LSTMLayer(descr_emb, HIDDEN_UNITS, grad_clipping=GRAD_CLIP)\n",
    "descr_lstm_flat = lasagne.layers.FlattenLayer(descr_lstm)\n",
    "\n",
    "# Titles\n",
    "title_emb = lasagne.layers.EmbeddingLayer(title_inp, input_size=len(token_to_id)+1, output_size=128)\n",
    "title_lstm = lasagne.layers.LSTMLayer(title_emb, HIDDEN_UNITS, grad_clipping=GRAD_CLIP)\n",
    "title_lstm_flat = lasagne.layers.FlattenLayer(title_lstm)\n",
    "\n",
    "# Non-sequences\n",
    "cat_dense_1 = lasagne.layers.DenseLayer(cat_inp, num_units=64, nonlinearity=lasagne.nonlinearities.leaky_rectify)\n",
    "cat_drop = lasagne.layers.DropoutLayer(cat_dense_1)\n",
    "cat_dense_2 = lasagne.layers.DenseLayer(cat_drop, num_units=64, nonlinearity=lasagne.nonlinearities.leaky_rectify)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# merge three layers into one (e.g. lasagne.layers.concat)\n",
    "l_concat = lasagne.layers.concat([descr_lstm_flat, title_lstm_flat, cat_dense_2])  \n",
    "\n",
    "l_dense = lasagne.layers.DenseLayer(l_concat, 1024)\n",
    "l_dropout = lasagne.layers.DropoutLayer(l_dense, p=0.05)\n",
    "dense_output = lasagne.layers.DenseLayer(l_dropout, 1, nonlinearity=lasagne.nonlinearities.linear)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Loss function\n",
    "\n",
    "* The standard way:\n",
    " * prediction\n",
    " * loss\n",
    " * updates\n",
    " * training and evaluation functions\n",
    " \n",
    " \n",
    "* Hinge loss\n",
    " * $ L_i = \\max(0, \\delta - t_i p_i) $\n",
    " * delta is a tunable parameter: how far should a neuron be in the positive margin area for us to stop bothering about it\n",
    " * Function description may mention some +-1  limitations - this is not neccessary, at least as long as hinge loss has a __default__ flag `binary = True`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#All trainable params\n",
    "weights = lasagne.layers.get_all_params(dense_output, trainable=True)\n",
    "\n",
    "#Simple NN prediction\n",
    "prediction = lasagne.layers.get_output(dense_output)[:,0]\n",
    "\n",
    "#Hinge loss\n",
    "loss = lasagne.objectives.binary_hinge_loss(prediction, target_y, delta=1.0, log_odds=True).mean()\n",
    "loss += 1e-5 * lasagne.regularization.regularize_layer_params(\n",
    "        [descr_lstm, title_lstm, cat_dense_2, l_dense], \n",
    "        lasagne.regularization.l2\n",
    "    )\n",
    "#Weight optimization step\n",
    "initial_lr = 0.01\n",
    "sh_lr = theano.shared(lasagne.utils.floatX(initial_lr))\n",
    "updates = lasagne.updates.adam(loss, params=weights, learning_rate=sh_lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Determinitic prediction \n",
    " * In case we use stochastic elements, e.g. dropout or noize\n",
    " * Compile a separate set of functions with deterministic prediction (deterministic = True)\n",
    " * Unless you think there's no neet for dropout there ofc. Btw is there?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#deterministic version\n",
    "det_prediction = lasagne.layers.get_output(dense_output, deterministic=True)[:,0]\n",
    "\n",
    "#equivalent loss function\n",
    "det_loss = lasagne.objectives.binary_hinge_loss(det_prediction, target_y, delta=1.0, log_odds=True).mean()\n",
    "det_loss += 1e-5 * lasagne.regularization.regularize_layer_params(\n",
    "        [descr_lstm, title_lstm, cat_dense_2, l_dense], \n",
    "        lasagne.regularization.l2\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Coffee-lation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "train_fun = theano.function([desc_token_ids, title_token_ids, categories, target_y], [loss, prediction], updates=updates)\n",
    "eval_fun = theano.function([desc_token_ids, title_token_ids, categories, target_y], [det_loss, det_prediction])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Training loop\n",
    "* The regular way with loops over minibatches\n",
    "* Since the dataset is huge, we define epoch as some fixed amount of samples isntead of all dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#average precision at K\n",
    "\n",
    "from oracle import APatK, score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Out good old minibatch iterator now supports arbitrary amount of arrays (X,y,z)\n",
    "\n",
    "def iterate_minibatches(*arrays, **kwargs):\n",
    "    batchsize=kwargs.get(\"batchsize\", 100)\n",
    "    shuffle = kwargs.get(\"shuffle\", True)\n",
    "    \n",
    "    if shuffle:\n",
    "        indices = np.arange(len(arrays[0]))\n",
    "        np.random.shuffle(indices)\n",
    "    for start_idx in range(0, len(arrays[0]) - batchsize + 1, batchsize):\n",
    "        if shuffle:\n",
    "            excerpt = indices[start_idx:start_idx + batchsize]\n",
    "        else:\n",
    "            excerpt = slice(start_idx, start_idx + batchsize)\n",
    "        yield [arr[excerpt] for arr in arrays]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Tweaking guide\n",
    "\n",
    "* batch_size - how many samples are processed per function call\n",
    "  * optimization gets slower, but more stable, as you increase it.\n",
    "  * May consider increasing it halfway through training\n",
    "* minibatches_per_epoch - max amount of minibatches per epoch\n",
    "  * Does not affect training. Lesser value means more frequent and less stable printing\n",
    "  * Setting it to less than 10 is only meaningfull if you want to make sure your NN does not break down after one epoch\n",
    "* n_epochs - total amount of epochs to train for\n",
    "  * `n_epochs = 10**10` and manual interrupting is still an option\n",
    "\n",
    "\n",
    "Tips:\n",
    "\n",
    "* With small minibatches_per_epoch, network quality may jump around 0.5 for several epochs\n",
    "\n",
    "* AUC is the most stable of all three metrics\n",
    "\n",
    "* Average Precision at top 2.5% (APatK) - is the least stable. If batch_size*minibatches_per_epoch < 10k, it behaves as a uniform random variable.\n",
    "\n",
    "* Plotting metrics over training time may be a good way to analyze which architectures work better.\n",
    "\n",
    "* Once you are sure your network aint gonna crash, it's worth letting it train for a few hours of an average laptop's time to see it's true potential"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_epoch:\t0\n",
      "Train:\n",
      "\tloss: 597.318714241\n",
      "\tacc: 0.831022635901\n",
      "\tauc: 0.859231996787\n",
      "\tap@k: 0.337125655038\n",
      "Val:\n",
      "\tloss: 1.65998297632\n",
      "\tacc: 0.840824975018\n",
      "\tauc: 0.915107416445\n",
      "\tap@k: 0.997366145613\n",
      "time spent:184.71901679039001\n",
      "n_epoch:\t1\n",
      "Train:\n",
      "\tloss: 1.44198791225\n",
      "\tacc: 0.906569040715\n",
      "\tauc: 0.938910910438\n",
      "\tap@k: 0.978793737512\n",
      "Val:\n",
      "\tloss: 0.620406056595\n",
      "\tacc: 0.869517746771\n",
      "\tauc: 0.920205765051\n",
      "\tap@k: 0.997549042435\n",
      "time spent:185.48784565925598\n",
      "n_epoch:\t2\n",
      "Train:\n",
      "\tloss: 0.377688922724\n",
      "\tacc: 0.926080017543\n",
      "\tauc: 0.950487769247\n",
      "\tap@k: 0.991618348646\n",
      "Val:\n",
      "\tloss: 0.559623729898\n",
      "\tacc: 0.812881675858\n",
      "\tauc: 0.829523388841\n",
      "\tap@k: 0.996560902506\n",
      "time spent:186.7937560081482\n",
      "n_epoch:\t3\n",
      "Train:\n",
      "\tloss: 0.304021805758\n",
      "\tacc: 0.925787627007\n",
      "\tauc: 0.946531549449\n",
      "\tap@k: 0.989475327168\n",
      "Val:\n",
      "\tloss: 0.444478763171\n",
      "\tacc: 0.835495392131\n",
      "\tauc: 0.874994140401\n",
      "\tap@k: 0.997615429525\n",
      "time spent:186.74020719528198\n",
      "n_epoch:\t4\n",
      "Train:\n",
      "\tloss: 0.364381064444\n",
      "\tacc: 0.882934139032\n",
      "\tauc: 0.901652030617\n",
      "\tap@k: 0.998722192615\n",
      "Val:\n",
      "\tloss: 0.561544893225\n",
      "\tacc: 0.782421629224\n",
      "\tauc: 0.809534094081\n",
      "\tap@k: 0.998283912735\n",
      "new_lr:0.0031999999191612005\n",
      "time spent:185.92602825164795\n",
      "n_epoch:\t5\n",
      "Train:\n",
      "\tloss: 0.29593439717\n",
      "\tacc: 0.906063448746\n",
      "\tauc: 0.920917311235\n",
      "\tap@k: 0.997451918409\n",
      "Val:\n",
      "\tloss: 0.452443188863\n",
      "\tacc: 0.819118028054\n",
      "\tauc: 0.839949766352\n",
      "\tap@k: 0.997838136988\n",
      "time spent:185.85588335990906\n",
      "n_epoch:\t6\n",
      "Train:\n",
      "\tloss: 0.228129351323\n",
      "\tacc: 0.92612265783\n",
      "\tauc: 0.940090551484\n",
      "\tap@k: 0.998419280741\n",
      "Val:\n",
      "\tloss: 0.408795940019\n",
      "\tacc: 0.84894888782\n",
      "\tauc: 0.867403559111\n",
      "\tap@k: 0.99817009024\n",
      "time spent:185.78944873809814\n",
      "n_epoch:\t7\n",
      "Train:\n",
      "\tloss: 0.180248627383\n",
      "\tacc: 0.941095489876\n",
      "\tauc: 0.955140122068\n",
      "\tap@k: 0.999445713989\n",
      "Val:\n",
      "\tloss: 0.3893106092\n",
      "\tacc: 0.846580184315\n",
      "\tauc: 0.864210494155\n",
      "\tap@k: 0.998761838209\n",
      "time spent:185.53817296028137\n",
      "n_epoch:\t8\n",
      "Train:\n",
      "\tloss: 0.15063085594\n",
      "\tacc: 0.952255062011\n",
      "\tauc: 0.965845241099\n",
      "\tap@k: 0.994208632701\n",
      "Val:\n",
      "\tloss: 0.476124484786\n",
      "\tacc: 0.817952181798\n",
      "\tauc: 0.836972340745\n",
      "\tap@k: 0.998738347998\n",
      "time spent:185.64756059646606\n",
      "n_epoch:\t9\n",
      "Train:\n",
      "\tloss: 0.182596780332\n",
      "\tacc: 0.937075120002\n",
      "\tauc: 0.950944559343\n",
      "\tap@k: 0.999395497985\n",
      "Val:\n",
      "\tloss: 0.387753733386\n",
      "\tacc: 0.864345460602\n",
      "\tauc: 0.911867184375\n",
      "\tap@k: 0.998910532653\n",
      "new_lr:0.0010239999974146485\n",
      "time spent:186.3067057132721\n",
      "n_epoch:\t10\n",
      "Train:\n",
      "\tloss: 0.153160932094\n",
      "\tacc: 0.950488535854\n",
      "\tauc: 0.962990140129\n",
      "\tap@k: 0.998111099862\n",
      "Val:\n",
      "\tloss: 0.34906808886\n",
      "\tacc: 0.86971205448\n",
      "\tauc: 0.888376606214\n",
      "\tap@k: 0.998748311306\n",
      "time spent:186.05194735527039\n",
      "n_epoch:\t11\n",
      "Train:\n",
      "\tloss: 0.144625232595\n",
      "\tacc: 0.952078409395\n",
      "\tauc: 0.963124999375\n",
      "\tap@k: 0.997422074889\n",
      "Val:\n",
      "\tloss: 0.323754017408\n",
      "\tacc: 0.868000296088\n",
      "\tauc: 0.888309154931\n",
      "\tap@k: 0.998912678523\n",
      "time spent:186.25893354415894\n",
      "n_epoch:\t12\n",
      "Train:\n",
      "\tloss: 0.137646595346\n",
      "\tacc: 0.953930216125\n",
      "\tauc: 0.964351675096\n",
      "\tap@k: 0.999293867406\n",
      "Val:\n",
      "\tloss: 0.328959479254\n",
      "\tacc: 0.874107109812\n",
      "\tauc: 0.891782179907\n",
      "\tap@k: 0.999179474541\n",
      "time spent:187.16269183158875\n",
      "n_epoch:\t13\n",
      "Train:\n",
      "\tloss: 0.13022776773\n",
      "\tacc: 0.956543456543\n",
      "\tauc: 0.966754129277\n",
      "\tap@k: 0.998977405853\n",
      "Val:\n",
      "\tloss: 0.328046543022\n",
      "\tacc: 0.877604648581\n",
      "\tauc: 0.895850878506\n",
      "\tap@k: 0.998801817436\n",
      "time spent:185.4951844215393\n",
      "n_epoch:\t14\n",
      "Train:\n",
      "\tloss: 0.126211309049\n",
      "\tacc: 0.958864306425\n",
      "\tauc: 0.96817409742\n",
      "\tap@k: 0.997717450381\n",
      "Val:\n",
      "\tloss: 0.31845772791\n",
      "\tacc: 0.880380473\n",
      "\tauc: 0.913200322801\n",
      "\tap@k: 0.998991274223\n",
      "new_lr:0.0003276799980085343\n",
      "time spent:185.3203101158142\n",
      "n_epoch:\t15\n",
      "Train:\n",
      "\tloss: 0.108982176914\n",
      "\tacc: 0.964858312419\n",
      "\tauc: 0.973546190811\n",
      "\tap@k: 0.998587383412\n",
      "Val:\n",
      "\tloss: 0.304571593769\n",
      "\tacc: 0.882573374292\n",
      "\tauc: 0.90042536315\n",
      "\tap@k: 0.999020362465\n",
      "time spent:185.453861951828\n",
      "n_epoch:\t16\n",
      "Train:\n",
      "\tloss: 0.104478116007\n",
      "\tacc: 0.966265441875\n",
      "\tauc: 0.974840479835\n",
      "\tap@k: 1.0\n",
      "Val:\n",
      "\tloss: 0.292842539974\n",
      "\tacc: 0.88425737444\n",
      "\tauc: 0.904685545296\n",
      "\tap@k: 0.998970433094\n",
      "time spent:185.30906772613525\n",
      "n_epoch:\t17\n",
      "Train:\n",
      "\tloss: 0.0949120809039\n",
      "\tacc: 0.970042152969\n",
      "\tauc: 0.978254335669\n",
      "\tap@k: 0.999425924682\n",
      "Val:\n",
      "\tloss: 0.293363451935\n",
      "\tacc: 0.888883748473\n",
      "\tauc: 0.906820841997\n",
      "\tap@k: 0.998752161488\n",
      "time spent:185.28575992584229\n",
      "n_epoch:\t18\n",
      "Train:\n",
      "\tloss: 0.0921152930176\n",
      "\tacc: 0.971936599985\n",
      "\tauc: 0.979676155414\n",
      "\tap@k: 0.992333694736\n",
      "Val:\n",
      "\tloss: 0.284824320686\n",
      "\tacc: 0.894194825863\n",
      "\tauc: 0.91330601929\n",
      "\tap@k: 0.999115190101\n",
      "time spent:185.6174819469452\n",
      "n_epoch:\t19\n",
      "Train:\n",
      "\tloss: 0.0814062583631\n",
      "\tacc: 0.974507200117\n",
      "\tauc: 0.982502443169\n",
      "\tap@k: 0.999912403245\n",
      "Val:\n",
      "\tloss: 0.247806576961\n",
      "\tacc: 0.906982123691\n",
      "\tauc: 0.926960600722\n",
      "\tap@k: 0.998952474646\n",
      "new_lr:0.00010485760139999911\n",
      "time spent:185.99004316329956\n",
      "n_epoch:\t20\n",
      "Train:\n",
      "\tloss: 0.0752903246552\n",
      "\tacc: 0.97693160498\n",
      "\tauc: 0.983632339474\n",
      "\tap@k: 0.999069605292\n",
      "Val:\n",
      "\tloss: 0.227259539381\n",
      "\tacc: 0.916068322292\n",
      "\tauc: 0.936898644151\n",
      "\tap@k: 0.998970848204\n",
      "time spent:185.97358298301697\n",
      "n_epoch:\t21\n",
      "Train:\n",
      "\tloss: 0.0718212343601\n",
      "\tacc: 0.977747861894\n",
      "\tauc: 0.984481984144\n",
      "\tap@k: 0.998773924577\n",
      "Val:\n",
      "\tloss: 0.231905758913\n",
      "\tacc: 0.912589289019\n",
      "\tauc: 0.931874693854\n",
      "\tap@k: 0.99958325367\n",
      "time spent:185.3564898967743\n",
      "n_epoch:\t22\n",
      "Train:\n",
      "\tloss: 0.0705317688128\n",
      "\tacc: 0.977760044833\n",
      "\tauc: 0.984701142317\n",
      "\tap@k: 0.997773273908\n",
      "Val:\n",
      "\tloss: 0.218018358878\n",
      "\tacc: 0.917928124653\n",
      "\tauc: 0.939159515706\n",
      "\tap@k: 0.999460152035\n",
      "time spent:186.356383562088\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-70-6e096bd18018>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mj\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mminibatches_per_epoch\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpred_probas\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_fun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb_desc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb_title\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb_cat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb_y\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0mb_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.4/site-packages/Theano-0.8.2-py3.4.egg/theano/compile/function_module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    857\u001b[0m         \u001b[0mt0_fn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    858\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 859\u001b[0;31m             \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    860\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    861\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'position_of_error'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.4/site-packages/Theano-0.8.2-py3.4.egg/theano/gof/op.py\u001b[0m in \u001b[0;36mrval\u001b[0;34m(p, i, o, n)\u001b[0m\n\u001b[1;32m    909\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mparams\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mgraph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mNoParams\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    910\u001b[0m             \u001b[0;31m# default arguments are stored in the closure of `rval`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 911\u001b[0;31m             \u001b[0;32mdef\u001b[0m \u001b[0mrval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnode_input_storage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mo\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnode_output_storage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    912\u001b[0m                 \u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mo\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    913\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mo\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mnode\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import roc_auc_score, accuracy_score\n",
    "import time\n",
    "\n",
    "n_epochs = 50\n",
    "batch_size = 164\n",
    "minibatches_per_epoch = 1000\n",
    "\n",
    "for i in range(n_epochs):\n",
    "    start_time = time.time()\n",
    "    #training\n",
    "    epoch_y_true = []\n",
    "    epoch_y_pred = []\n",
    "    \n",
    "    b_c = b_loss = 0\n",
    "    for j, (b_desc,b_title,b_cat, b_y) in enumerate(\n",
    "        iterate_minibatches(desc_tr, title_tr, nontext_tr.as_matrix(), target_tr, batchsize=batch_size, shuffle=True)):\n",
    "        if j > minibatches_per_epoch:break\n",
    "            \n",
    "        loss, pred_probas = train_fun(b_desc, b_title, b_cat, b_y)\n",
    "        \n",
    "        b_loss += loss\n",
    "        b_c +=1\n",
    "        \n",
    "        epoch_y_true.append(b_y)\n",
    "        epoch_y_pred.append(pred_probas)\n",
    "    \n",
    "    epoch_y_true = np.concatenate(epoch_y_true)\n",
    "    epoch_y_pred = np.concatenate(epoch_y_pred)\n",
    "    \n",
    "    print(\"n_epoch:\\t{}\".format(i))\n",
    "    print(\"Train:\")\n",
    "    print('\\tloss:', b_loss/b_c)\n",
    "    print('\\tacc:', accuracy_score(epoch_y_true,epoch_y_pred>0.))\n",
    "    print('\\tauc:', roc_auc_score(epoch_y_true,epoch_y_pred))\n",
    "    print('\\tap@k:', APatK(epoch_y_true,epoch_y_pred,K = int(len(epoch_y_pred)*0.025)+1))\n",
    "    \n",
    "    #evaluation\n",
    "    epoch_y_true = []\n",
    "    epoch_y_pred = []\n",
    "    b_c = b_loss = 0\n",
    "    for j, (b_desc,b_title,b_cat, b_y) in enumerate(\n",
    "        iterate_minibatches(desc_ts, title_ts, nontext_tr.as_matrix(), target_ts, batchsize=batch_size, shuffle=True)):\n",
    "        if j > minibatches_per_epoch: break\n",
    "        loss,pred_probas = eval_fun(b_desc,b_title,b_cat,b_y)\n",
    "        \n",
    "        b_loss += loss\n",
    "        b_c +=1\n",
    "        \n",
    "        epoch_y_true.append(b_y)\n",
    "        epoch_y_pred.append(pred_probas)\n",
    "\n",
    "    epoch_y_true = np.concatenate(epoch_y_true)\n",
    "    epoch_y_pred = np.concatenate(epoch_y_pred)\n",
    "    print(\"Val:\")\n",
    "    print('\\tloss:', b_loss/b_c)\n",
    "    print('\\tacc:', accuracy_score(epoch_y_true,epoch_y_pred>0.))\n",
    "    print('\\tauc:', roc_auc_score(epoch_y_true,epoch_y_pred))\n",
    "    print('\\tap@k:', APatK(epoch_y_true,epoch_y_pred,K = int(len(epoch_y_pred)*0.025)+1))\n",
    "    \n",
    "    if (i + 1) % 5 == 0:\n",
    "        learning_rate = sh_lr.get_value()\n",
    "        learning_rate *= 0.32\n",
    "        print(\"new_lr:{}\".format(learning_rate))\n",
    "        sh_lr.set_value(lasagne.utils.floatX(learning_rate))\n",
    "    print(\"time spent:{}\".format(time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "If you are seeing this, it's time to backup your notebook. No, really, 'tis too easy to mess up everything without noticing. \n"
     ]
    }
   ],
   "source": [
    "print(\"If you are seeing this, it's time to backup your notebook. No, really, 'tis too easy to mess up everything without noticing. \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Final evaluation\n",
    "Evaluate network over the entire test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scores:\n",
      "\tloss: 0.234091943351\n",
      "\tacc: 0.910933047115\n",
      "\tauc: 0.930724345809\n",
      "\tap@k: 0.999445624337\n",
      "\n",
      "AUC:\n",
      "\tНеплохо, но ты можешь лучше! (not ok)\n",
      "\n",
      "Accuracy:\n",
      "\tВсё ок (ok)\n",
      "\n",
      "Average precision at K:\n",
      "\tЗасабмить на kaggle! (great) \n",
      "\t Нет, ну честно - выкачай avito_test.tsv, засабмить и скажи, что вышло.\n"
     ]
    }
   ],
   "source": [
    "#evaluation\n",
    "epoch_y_true = []\n",
    "epoch_y_pred = []\n",
    "\n",
    "b_c = b_loss = 0\n",
    "for j, (b_desc, b_title, b_cat, b_y) in enumerate(\n",
    "    iterate_minibatches(desc_ts, title_ts, nontext_tr.as_matrix(), target_ts, batchsize=batch_size, shuffle=True)):\n",
    "    loss,pred_probas = eval_fun(b_desc, b_title, b_cat, b_y)\n",
    "\n",
    "    b_loss += loss\n",
    "    b_c +=1\n",
    "\n",
    "    epoch_y_true.append(b_y)\n",
    "    epoch_y_pred.append(pred_probas)\n",
    "\n",
    "\n",
    "epoch_y_true = np.concatenate(epoch_y_true)\n",
    "epoch_y_pred = np.concatenate(epoch_y_pred)\n",
    "\n",
    "final_accuracy = accuracy_score(epoch_y_true, epoch_y_pred>0)\n",
    "final_auc = roc_auc_score(epoch_y_true, epoch_y_pred)\n",
    "final_apatk = APatK(epoch_y_true, epoch_y_pred, K = int(len(epoch_y_pred)*0.025)+1)\n",
    "\n",
    "print(\"Scores:\")\n",
    "print('\\tloss:', b_loss/b_c)\n",
    "print('\\tacc:', final_accuracy)\n",
    "print('\\tauc:', final_auc)\n",
    "print('\\tap@k:', final_apatk)\n",
    "score(final_accuracy, final_auc, final_apatk)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Main task\n",
    "\n",
    "* https://goo.gl/forms/eJwIeAbjxzVuo6vn1\n",
    "* Feel like Le'Cun:\n",
    " * accuracy > 0.95\n",
    " * AUC > 0.97\n",
    " * Average Precision at (test sample size * 0.025) > 0.99\n",
    " * And perhaps even farther\n",
    "\n",
    "* Casual mode\n",
    " * accuracy > 0.90\n",
    " * AUC > 0.95\n",
    " * Average Precision at (test sample size * 0.025) > 0.92\n",
    "\n",
    "* Remember the training, Luke\n",
    " * Dropout, regularization\n",
    " * Mommentum, RMSprop, ada*\n",
    " * etc etc etc\n",
    " \n",
    " * If you have background in texts, there may be a way to improve tokenizer, add some lemmatization, etc etc.\n",
    " * In case you know how not to shoot yourself in the foot with RNNs, they too may be of some use."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
